{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75639070566390ce",
   "metadata": {},
   "source": [
    "At this stage, you have a dataset consisting of six folders, each containing a JSON annotation file and the corresponding images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80d43a6051c7b9",
   "metadata": {},
   "source": [
    "We will restructure our dataset into a new format suitable for training, containing 2 folders: one named \"Train\" where our model will be trained (containing 5 of the 6 previous folders with all images and a single JSON file in COCO format combining all annotations), and one folder named \"Val\" which will be used to evaluate our model's performance after each training epoch (containing the COCO format JSON file and images from the one original dataset folder not included in \"Train\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7813c66dffd48",
   "metadata": {},
   "source": [
    "The ultimate objective is to implement K-fold cross-validation, meaning we will perform 6 different training runs, rotating the \"Val\" folder so that each of our original dataset folders serves as the evaluation set during one training run, ensuring our model demonstrates consistent performance across all 6 dataset configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc722bfb227105fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.ndimage import map_coordinates\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import wandb\n",
    "from noise import pnoise2\n",
    "from sympy.benchmarks.bench_discrete_log import data_set_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ae83226f3c29a",
   "metadata": {},
   "source": [
    "As always, we implement a class to handle annotations in COCO format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6edd56c2c556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, coco_json, img_dir, img_list=None, transform=None, sigma=2, target_size=(512,512)):\n",
    "        with open(coco_json, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.sigma = sigma\n",
    "        self.target_size = target_size\n",
    "\n",
    "        \n",
    "        self.liste_id = {ann['id'] for ann in self.coco_data['annotations']}\n",
    "\n",
    "        \n",
    "        self.id_to_id_image_id = {ann['id']: ann['image_id'] for ann in self.coco_data['annotations']}\n",
    "\n",
    "        \n",
    "        self.img_id_to_file = {img['id']: img['file_name'] for img in self.coco_data['images']}\n",
    "\n",
    "        \n",
    "        self.id_to_keypoints = {ann['id']: ann['keypoints'] for ann in self.coco_data['annotations']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8911524252d8c79",
   "metadata": {},
   "source": [
    "Please indicate which training run you are on out of the 6 (in range 0 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b372e3ec852444",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_iter = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d713b4d8fdb888",
   "metadata": {},
   "source": [
    "Let's create a list containing all the paths to your annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce39fcdec54da1",
   "metadata": {},
   "source": [
    "Please specify the path to your dataset in 'racine ='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "racine = 'DATASET'\n",
    "fichiers = []\n",
    "fichiers_json = []\n",
    "\n",
    "for element in os.listdir(racine):\n",
    "    chemin_dossier = os.path.join(racine, element)\n",
    "    if os.path.isdir(chemin_dossier):\n",
    "        fichiers.append(chemin_dossier)\n",
    "        chemin_json = os.path.join(chemin_dossier, 'annotations.json')\n",
    "        if os.path.exists(chemin_json):\n",
    "            fichiers_json.append(chemin_json)\n",
    "\n",
    "print(fichiers)\n",
    "print(fichiers_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71625793beb8a",
   "metadata": {},
   "source": [
    "Let's now create, with these next two cells, two annotation files in your dataset: one corresponding to the evaluation data and the other to the training data (which is a concatenation of annotations from 5 out of 6 of your original annotation files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91d970eaa2fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers_json_train = fichiers_json[:k_iter] + fichiers_json[k_iter+1:]\n",
    "\n",
    "fichiers_train = fichiers[:k_iter] + fichiers[k_iter+1:]\n",
    "\n",
    "fichier_json_val = fichiers_json[k_iter]\n",
    "\n",
    "fichier_val = fichiers[k_iter]\n",
    "source = fichiers_json_train[0]\n",
    "\n",
    "\n",
    "new_filename = \"annotations_train.json\"\n",
    "os.makedirs(racine, exist_ok=True)\n",
    "destination = os.path.join(racine, new_filename)\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "\n",
    "source = fichier_json_val\n",
    "new_filename = \"annotations_val.json\"\n",
    "destination = os.path.join(racine, new_filename)\n",
    "shutil.copyfile(source, destination)\n",
    "\n",
    "print(fichiers_json_train)\n",
    "print(fichier_json_val)\n",
    "print(fichiers_train)\n",
    "print(fichier_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddeab095079805",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = racine + \"/annotations_train.json\"\n",
    "\n",
    "with open(annotations_path, 'r') as f:\n",
    "    data_main = json.load(f)\n",
    "\n",
    "max_img_id = max([img['id'] for img in data_main.get('images', [])], default=0)\n",
    "max_ann_id = max([ann['id'] for ann in data_main.get('annotations', [])], default=0)\n",
    "\n",
    "for fichier_path in fichiers_json_train[1:]:\n",
    "    if not os.path.exists(fichier_path):\n",
    "        print(f\"Fichier introuvable : {fichier_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(fichier_path, 'r') as f:\n",
    "            data_to_merge = json.load(f)\n",
    "\n",
    "        if 'images' not in data_to_merge or 'annotations' not in data_to_merge:\n",
    "            print(f\"Structure invalide dans : {fichier_path}\")\n",
    "            continue\n",
    "\n",
    "        img_id_mapping = {}\n",
    "        ann_id_mapping = {}\n",
    "\n",
    "\n",
    "        for img in data_to_merge.get('images', []):\n",
    "            old_id = img['id']\n",
    "            max_img_id += 1\n",
    "            new_id = max_img_id\n",
    "\n",
    "            img_id_mapping[old_id] = new_id\n",
    "            img['id'] = new_id\n",
    "\n",
    "        for ann in data_to_merge.get('annotations', []):\n",
    "            old_ann_id = ann['id']\n",
    "            old_img_id = ann['image_id']\n",
    "\n",
    "            max_ann_id += 1\n",
    "            ann['id'] = max_ann_id\n",
    "            ann['image_id'] = img_id_mapping.get(old_img_id, old_img_id)\n",
    "\n",
    "\n",
    "        data_main['images'].extend(data_to_merge.get('images', []))\n",
    "        data_main['annotations'].extend(data_to_merge.get('annotations', []))\n",
    "\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erreur JSON dans : {fichier_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {fichier_path}: {e}\")\n",
    "\n",
    "with open(annotations_path, 'w') as f:\n",
    "    json.dump(data_main, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d6b8a5ad3e7bd",
   "metadata": {},
   "source": [
    "Run this next cell to have two folders, one VAL and the other TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a2d09d9334d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(racine, \"TRAIN\")\n",
    "os.rename(fichiers_train[0], train_dir)\n",
    "\n",
    "\n",
    "val_dir = os.path.join(racine, \"VAL\")\n",
    "os.rename(fichier_val, val_dir)\n",
    "\n",
    "for i in range(1, len(fichiers_train)):\n",
    "    source_dir = fichiers_train[i]\n",
    "\n",
    "    for item in os.listdir(source_dir):\n",
    "        source_item = os.path.join(source_dir, item)\n",
    "        dest_item = os.path.join(train_dir, item)\n",
    "\n",
    "        if os.path.isdir(source_item):\n",
    "\n",
    "            shutil.copytree(source_item, dest_item)\n",
    "        else:\n",
    "\n",
    "            shutil.copy2(source_item, dest_item)\n",
    "\n",
    "\n",
    "    shutil.rmtree(source_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed2746494fd087",
   "metadata": {},
   "source": [
    "Now you just need to delete all the files that are not images in \"TRAIN\" and \"VAL\", and drag the annotations_train file into \"TRAIN\" and annotations_val into \"VAL\", and your Dataset will be ready for training!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
