{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dc8ce05b894878",
   "metadata": {},
   "source": [
    "This notebook aims to standardize the dataset on which we will conduct the study, meaning converting it into a uniform format: a dataset consisting of\n",
    "six folders, each containing images and their annotations in COCO format.\n",
    "An important part of this notebook corresponds to how I personally proceeded to obtain the desired dataset format from the original dataset format I had available. It's up to you to see if certain cells in this notebook can also be useful for you to obtain the correct dataset format!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbff31e5f8fcefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import datetime\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16d95fd23f9737",
   "metadata": {},
   "source": [
    "I had a dataset consisting of six folders, each containing images and two JSON annotation files.\n",
    "The first was in COCO format and only provided the positions of the annotations, without considering the cell class (black cell or white cell).\n",
    "The second, in addition to the positions, included a binary grid indicating the two possible classes: black cell or white cell.\n",
    "The two classes below are designed to handle the data from these two JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, coco_json, img_dir, img_list=None, transform=None, sigma=2, target_size=(512,512)):\n",
    "        with open(coco_json, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.sigma = sigma\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.liste_id = {ann['id'] for ann in self.coco_data['annotations']}\n",
    "\n",
    "        self.id_to_id_image_id = {ann['id']: ann['image_id'] for ann in self.coco_data['annotations']}\n",
    "\n",
    "        self.img_id_to_file = {img['id']: img['file_name'] for img in self.coco_data['images']}\n",
    "\n",
    "        self.id_to_keypoints = {ann['id']: ann['keypoints'] for ann in self.coco_data['annotations']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0122317785357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryGrid(Dataset):\n",
    "    def __init__(self, json_dataset):\n",
    "        with open(json_dataset, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "\n",
    "        self.filename_to_id_dataset = {img['file_name']: img['id'] for img in self.data['images']}\n",
    "\n",
    "        self.id_dataset_to_binary_grid = {ann['image_id']: ann['datamatrix']['binary_grid'] for ann in self.data['annotations']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7a72c67048ded",
   "metadata": {},
   "source": [
    "I copy the dataset I was given to my personal workspace so that I can later convert it to the proper format and apply my transformations and other processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7869f0a7b0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '../../clone/NextGenCode_Dataset/NextGenCode_ROI_Dataset'\n",
    "destination_dir = 'DATASET'\n",
    "\n",
    "if os.path.exists(destination_dir):\n",
    "    shutil.rmtree(destination_dir)\n",
    "\n",
    "shutil.copytree(source_dir, destination_dir)\n",
    "\n",
    "for element in os.listdir(destination_dir):\n",
    "    dossier = os.path.join(destination_dir, element)\n",
    "    if os.path.isdir(dossier):\n",
    "        coco = 'processed_coco_annotations.json'\n",
    "        fichier = os.path.join(dossier, coco)\n",
    "        if os.path.isfile(fichier):\n",
    "            shutil.copy(fichier, os.path.join(dossier, 'annotations.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8f988d199900",
   "metadata": {},
   "source": [
    "In the annotation file containing the binary grids, the way of distinguishing the annotation classes was not standardized.\n",
    "For example, a 1 in the grid of one image could represent a black cell, while in another image it could represent a white cell.\n",
    "Therefore, it was necessary to standardize the annotation encoding method across all images. These functions call the two classes defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee52d91b2b0bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grille_binaire(id, file_name):\n",
    "    path_json = os.path.join(file_name, 'processed_annotations.json')\n",
    "    path_coco_json = os.path.join(file_name, 'annotations.json')\n",
    "    X = BinaryGrid(json_dataset=path_json)\n",
    "    Y = KeypointDataset(coco_json=path_coco_json, img_dir=file_name)\n",
    "\n",
    "    id_image = Y.id_to_id_image_id[id]\n",
    "    filename = Y.img_id_to_file[id_image]\n",
    "    id_dataset = X.filename_to_id_dataset[filename]\n",
    "    binary_grid = X.id_dataset_to_binary_grid[id_dataset]\n",
    "    return binary_grid\n",
    "\n",
    "\n",
    "def liste_binaire(grille_binaire):\n",
    "    liste_binaire = []\n",
    "    for i in grille_binaire:\n",
    "        liste_binaire.extend(i)\n",
    "    return liste_binaire\n",
    "\n",
    "def coherent(grille_binaire):\n",
    "    l0 = [row[0] for row in grille_binaire]\n",
    "    l1 = grille_binaire[0]\n",
    "    l2 = [row[-1] for row in grille_binaire]\n",
    "    l3 = grille_binaire[-1]\n",
    "    l = [l0, l1, l2, l3]\n",
    "    retour = False\n",
    "    for i in range(4):\n",
    "        j = (i + 1) % 4\n",
    "        somme = sum(l[i]) + sum(l[j])\n",
    "        longueur = len(l[i]) + len(l[j])\n",
    "        if somme == longueur:\n",
    "            retour = True\n",
    "    return retour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd24e44b89b0f3",
   "metadata": {},
   "source": [
    "Here, we iterate through the annotations by ID and only update the keypoints in our COCO-format JSON file to follow this distinction:\n",
    "v = 2 for black cells and v = 1 for white cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580363b23a946d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = destination_dir\n",
    "\n",
    "for element in os.listdir(dataset):\n",
    "    image_dir = os.path.join(dataset, element)\n",
    "    path_coco_json = os.path.join(image_dir, 'annotations.json')\n",
    "    path_json = os.path.join(image_dir, 'processed_annotations.json')\n",
    "\n",
    "    if os.path.isfile(path_coco_json) and os.path.isfile(path_json):\n",
    "        X = BinaryGrid(json_dataset=path_json)\n",
    "        Y = KeypointDataset(coco_json=path_coco_json, img_dir=image_dir)\n",
    "\n",
    "        with open(path_coco_json, 'r') as f:\n",
    "            data_set = json.load(f)\n",
    "\n",
    "        for id in Y.liste_id:\n",
    "            try:\n",
    "                keypoints = Y.id_to_keypoints[id]\n",
    "                grille_binaire_result = grille_binaire(id, image_dir)\n",
    "                liste_binaire_result = liste_binaire(grille_binaire_result)\n",
    "\n",
    "                if coherent(grille_binaire_result):\n",
    "                    for i in range(0, len(keypoints), 3):\n",
    "                        if liste_binaire_result[i // 3] == 1:\n",
    "                            keypoints[i + 2] = 2\n",
    "                        else:\n",
    "                            keypoints[i + 2] = 1\n",
    "                else:\n",
    "                    for i in range(0, len(keypoints), 3):\n",
    "                        if liste_binaire_result[i // 3] == 1:\n",
    "                            keypoints[i + 2] = 1\n",
    "                        else:\n",
    "                            keypoints[i + 2] = 2\n",
    "\n",
    "                for ann in data_set['annotations']:\n",
    "                    if ann['id'] == id:\n",
    "                        ann['keypoints'] = keypoints\n",
    "            except (IndexError, ValueError) as e:\n",
    "                print(f\"Error processing id {id}: {e}\")\n",
    "\n",
    "        with open(path_coco_json, 'w') as f:\n",
    "            json.dump(data_set, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f2af1f925495e",
   "metadata": {},
   "source": [
    "At this stage, each dataset folder contains an \"annotations.json\" file in COCO format that distinguishes datamatrix cell classes (v=1 for white cells, v=2 for black cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f3047f1feb6bb",
   "metadata": {},
   "source": [
    "In case you only have an image folder in your Dataset, you can execute this next cell to obtain a Dataset in the format. Once this format is obtained, it's up to you to see if the cells above can be useful to you. Please specify: input_folder = your dataset, input_annotations = your annotations, out_folder = the path to your future Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91635f691be108c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder =\n",
    "input_annotations =\n",
    "output_folder =\n",
    "n_clusters =\n",
    "\n",
    "\n",
    "def segment_images(input_folder, output_folder=\"segmented_images\", n_clusters=5):\n",
    "    input_path = Path(input_folder)\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    image_files = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "        image_files.extend(input_path.glob(ext))\n",
    "        image_files.extend(input_path.glob(ext.upper()))\n",
    "\n",
    "    features = []\n",
    "    valid_images = []\n",
    "\n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(img, (64, 64))  \n",
    "\n",
    "            mean_colors = np.mean(img.reshape(-1, 3), axis=0)\n",
    "            std_colors = np.std(img.reshape(-1, 3), axis=0)\n",
    "\n",
    "            feature = np.concatenate([mean_colors, std_colors])\n",
    "            features.append(feature)\n",
    "            valid_images.append(img_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(features) < n_clusters:\n",
    "        n_clusters = len(features)\n",
    "\n",
    "    features_array = np.array(features)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(features_array)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        cluster_folder = output_path / f\"groupe_{i+1}\"\n",
    "        cluster_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    for img_path, label in zip(valid_images, labels):\n",
    "        dest_folder = output_path / f\"groupe_{label+1}\"\n",
    "        dest_path = dest_folder / img_path.name\n",
    "\n",
    "        counter = 1\n",
    "        while dest_path.exists():\n",
    "            stem = img_path.stem\n",
    "            suffix = img_path.suffix\n",
    "            dest_path = dest_folder / f\"{stem}_{counter}{suffix}\"\n",
    "            counter += 1\n",
    "\n",
    "        shutil.copy2(img_path, dest_path)\n",
    "\n",
    "\n",
    "\n",
    "segment_images(input_folder, output_folder, n_clusters)\n",
    "\n",
    "\n",
    "Z = KeypointDataset(coco_json=input_annotations, img_dir=input_folder)\n",
    "\n",
    "\n",
    "path_coco_json = input_annotations\n",
    "\n",
    "filename_to_id = {filename: img_id for img_id, filename in Z.img_id_to_file.items()}\n",
    "\n",
    "\n",
    "output_path = Path(output_folder)\n",
    "for cluster_folder in output_path.iterdir():\n",
    "    if cluster_folder.is_dir():\n",
    "        annotations = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": Z.coco_data.get(\"categories\", []),\n",
    "            \"info\": Z.coco_data.get(\"info\", {}),\n",
    "            \"licenses\": Z.coco_data.get(\"licenses\", [])\n",
    "        }\n",
    "\n",
    "\n",
    "        for img_file in cluster_folder.iterdir():\n",
    "            if img_file.is_file() and img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "\n",
    "                original_filename = img_file.name\n",
    "\n",
    "                if '_' in img_file.stem and img_file.stem.split('_')[-1].isdigit():\n",
    "                    stem_parts = img_file.stem.split('_')[:-1]\n",
    "                    original_filename = '_'.join(stem_parts) + img_file.suffix\n",
    "\n",
    "\n",
    "                if original_filename in filename_to_id:\n",
    "                    img_id = filename_to_id[original_filename]\n",
    "\n",
    "\n",
    "                    image_info = None\n",
    "                    for img in Z.coco_data['images']:\n",
    "                        if img['id'] == img_id:\n",
    "                            image_info = img.copy()\n",
    "                            break\n",
    "\n",
    "                    if image_info:\n",
    "\n",
    "                        annotations[\"images\"].append(image_info)\n",
    "\n",
    "\n",
    "                        for ann in Z.coco_data['annotations']:\n",
    "                            if ann['image_id'] == img_id:\n",
    "\n",
    "                                annotations[\"annotations\"].append(ann.copy())\n",
    "\n",
    "\n",
    "        output_json_path = cluster_folder / \"annotations.json\"\n",
    "        with open(output_json_path, 'w') as f:\n",
    "            json.dump(annotations, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38293e744447b2ce",
   "metadata": {},
   "source": [
    "Finally, we now have a dataset consisting of six folders, each containing DataMatrix images and a COCO-format JSON file with the annotations of the DataMatrix present in the images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
